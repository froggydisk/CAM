{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7823d0e217dc4479b6a7d9ecaa603868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_149263755d0347f3a3cfb9931e451ff6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_846351e3802d4623822169a7af7382fc",
              "IPY_MODEL_11eb327694864c69ba279a00ce7b4746"
            ]
          }
        },
        "149263755d0347f3a3cfb9931e451ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "846351e3802d4623822169a7af7382fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1450a7857a484a2eb6d5d53764115e00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b63574a946e945aa99a9571d595f0cb5"
          }
        },
        "11eb327694864c69ba279a00ce7b4746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d736cc40f5e46b4bb4fa955a44a8b8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 17570570.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_804c5cacf2dd44fc8de4504addd0836b"
          }
        },
        "1450a7857a484a2eb6d5d53764115e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b63574a946e945aa99a9571d595f0cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d736cc40f5e46b4bb4fa955a44a8b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "804c5cacf2dd44fc8de4504addd0836b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzT_wF16DBLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, img_size, num_class):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3,32,3,1,1),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,3,1,1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64,128,3,1,1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128,256,3,1,1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(256,512,3,1,1),\n",
        "            nn.ReLU())\n",
        "    \n",
        "        self.gap = nn.AvgPool2d(img_size // 8)\n",
        "        \n",
        "        self.classifier = nn.Linear(512, num_class)\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        feature = self.layer5(out)\n",
        "        out = self.gap(feature).view(feature.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out, feature"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1dni4aqDEMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Util\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def set_trainloader(dataset_name, path, img_size, batch_size):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize(img_size),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),])\n",
        "    if dataset_name == 'MNIST':\n",
        "      dataset = dsets.MNIST(root=path, train=True, download=True, transform=transform)\n",
        "    elif dataset_name == 'CIFAR10':\n",
        "      dataset = dsets.CIFAR10(root=path, train=True, download=True, transform=transform)\n",
        "    else:\n",
        "      dataset = dsets.ImageFolder(root=path, transform=transform)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2), len(dataset.classes)\n",
        "\n",
        "def set_testloader(dataset_name, path, img_size, batch_size=1):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),])\n",
        "    if dataset_name == 'MNIST':\n",
        "      dataset = dsets.MNIST(root=path, train=False, download=True, transform=transform)\n",
        "    elif dataset_name == 'CIFAR10':\n",
        "      dataset = dsets.CIFAR10(root=path, train=False, download=True, transform=transform)\n",
        "    else:\n",
        "      dataset = dsets.ImageFolder(root=path, transform=transform)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2), len(dataset.classes)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTql5jYeDHE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "7823d0e217dc4479b6a7d9ecaa603868",
            "149263755d0347f3a3cfb9931e451ff6",
            "846351e3802d4623822169a7af7382fc",
            "11eb327694864c69ba279a00ce7b4746",
            "1450a7857a484a2eb6d5d53764115e00",
            "b63574a946e945aa99a9571d595f0cb5",
            "8d736cc40f5e46b4bb4fa955a44a8b8a",
            "804c5cacf2dd44fc8de4504addd0836b"
          ]
        },
        "outputId": "6008f34a-b6bc-40e9-fe80-3318cf8e0cc0"
      },
      "source": [
        "# Train with CIFAR10\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "dataset = 'CIFAR10'\n",
        "data_path = './data'\n",
        "model_path = './model'\n",
        "model_name = './CAMnet.pth'\n",
        "img_size = 32\n",
        "batch_size = 128\n",
        "epoch = 5\n",
        "epoch_box = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "  os.mkdir(model_path)\n",
        "\n",
        "train_loader, num_class = set_trainloader(dataset, data_path, img_size, batch_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net = CNN(img_size=img_size, num_class=num_class).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "min_loss = 100\n",
        "\n",
        "print(\"Training Starts\")\n",
        "for ep in range(epoch):\n",
        "  running_loss = 0.0\n",
        "  total_loss = 0.0\n",
        "  for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs, _ = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    total_loss += loss.item()\n",
        "    if i % epoch_box == epoch_box-1:\n",
        "      print('Epoch [%d/%d], Iteration [%d/%d], loss: %.4f' % \n",
        "            (ep+1, epoch, i+1, len(train_loader), running_loss/epoch_box))\n",
        "      running_loss = 0.0\n",
        "  epoch_loss = total_loss/len(train_loader)\n",
        "  print('Epoch [%d/%d], Total Loss: %.4f' % (ep+1, epoch, epoch_loss))\n",
        "  \n",
        "  if epoch_loss < min_loss:\n",
        "    min_loss = epoch_loss\n",
        "    torch.save(net.state_dict(), os.path.join(model_path, model_name))\n",
        "print('Training Finished')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7823d0e217dc4479b6a7d9ecaa603868",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Training Starts\n",
            "Epoch [1/5], Iteration [100/390], loss: 2.0521\n",
            "Epoch [1/5], Iteration [200/390], loss: 1.7433\n",
            "Epoch [1/5], Iteration [300/390], loss: 1.5532\n",
            "Epoch [1/5], Total Loss: 1.7039\n",
            "Epoch [2/5], Iteration [100/390], loss: 1.3529\n",
            "Epoch [2/5], Iteration [200/390], loss: 1.2555\n",
            "Epoch [2/5], Iteration [300/390], loss: 1.1685\n",
            "Epoch [2/5], Total Loss: 1.2265\n",
            "Epoch [3/5], Iteration [100/390], loss: 1.0502\n",
            "Epoch [3/5], Iteration [200/390], loss: 1.0298\n",
            "Epoch [3/5], Iteration [300/390], loss: 0.9693\n",
            "Epoch [3/5], Total Loss: 0.9988\n",
            "Epoch [4/5], Iteration [100/390], loss: 0.8860\n",
            "Epoch [4/5], Iteration [200/390], loss: 0.8745\n",
            "Epoch [4/5], Iteration [300/390], loss: 0.8574\n",
            "Epoch [4/5], Total Loss: 0.8617\n",
            "Epoch [5/5], Iteration [100/390], loss: 0.7844\n",
            "Epoch [5/5], Iteration [200/390], loss: 0.7575\n",
            "Epoch [5/5], Iteration [300/390], loss: 0.7408\n",
            "Epoch [5/5], Total Loss: 0.7564\n",
            "Training Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZbuX5aWDJoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fada816b-b7f3-4ca1-f00c-2be2b5c8b492"
      },
      "source": [
        "# Test with CIFAR10\n",
        "import torch\n",
        "import numpy as np\n",
        "dataset = 'CIFAR10'\n",
        "data_path = './data'\n",
        "model_path = './model'\n",
        "model_name = 'CAMnet.pth'\n",
        "img_size = 32\n",
        "\n",
        "test_loader, num_class = set_testloader(dataset, data_path, img_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "net = CNN(img_size=img_size, num_class=num_class).to(device)\n",
        "net.load_state_dict(torch.load(os.path.join(model_path, model_name)))\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs, _ = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 72 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUbr2qL1DLnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5cec5666-93fe-4167-f144-c2f7e039f44c"
      },
      "source": [
        "# CAM with CIFAR10\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "dataset = 'CIFAR10'\n",
        "data_path = './data'\n",
        "model_path = './model'\n",
        "model_name = 'CAMnet.pth'\n",
        "result_path = './result'\n",
        "img_size = 32\n",
        "result_num = 3\n",
        "\n",
        "\n",
        "if not os.path.exists(result_path):\n",
        "    os.mkdir(result_path)\n",
        "\n",
        "test_loader, num_class = set_testloader(dataset, data_path, img_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "net = CNN(img_size=img_size, num_class=num_class).to(device)\n",
        "net.load_state_dict(torch.load(os.path.join(model_path, model_name)))\n",
        "\n",
        "feature_collection = []\n",
        "def get_feature(input):\n",
        "  _, feature = net(input)\n",
        "  feature_collection.append(feature.cpu().data.numpy())\n",
        "\n",
        "params = list(net.parameters())\n",
        "weight_for_softmax = np.squeeze(params[-2].cpu().data.numpy())\n",
        "\n",
        "def Do_CAM(feature, weigth_for_softmax, class_id):\n",
        "  upsample_size = (img_size, img_size)\n",
        "  _, c, h, w = feature.shape\n",
        "  cam = np.dot(weight_for_softmax[class_id],feature.reshape(c, h*w))\n",
        "  cam = cam.reshape(h, w)\n",
        "  cam = (cam - np.min(cam)) / np.max(cam)\n",
        "  cam = np.uint8(255 * cam)\n",
        "  cv2.resize(cam, upsample_size)\n",
        "  return cam\n",
        "\n",
        "for i, (image, label) in enumerate(test_loader):\n",
        "  PIL_image = transforms.ToPILImage()(image[0])\n",
        "  PIL_image.save(os.path.join(result_path, 'img%d.png' %(i+1)))\n",
        "  image, label = image.to(device), label.to(device)\n",
        "  get_feature(image)\n",
        "  out, _ = net(image)\n",
        "  Sc = F.softmax(out, dim=1).data.squeeze()\n",
        "  prob, id = Sc.sort(0,True)\n",
        "  print(\"GT : %d, Pred : %d, Prob : %.2f\" % (label.item(), id[0].item(), prob[0].item()))\n",
        "  CAM = Do_CAM(feature_collection[0], weight_for_softmax, id[0].item())\n",
        "  image = cv2.imread(os.path.join(result_path, 'img%d.png' % (i+1)))\n",
        "  height, width, _ = image.shape\n",
        "  heatmap = cv2.applyColorMap(cv2.resize(CAM, (width, height)), cv2.COLORMAP_JET)\n",
        "  result = heatmap*0.5 + image*0.5\n",
        "  cv2.imwrite(os.path.join(result_path, 'cam%d.png' % (i+1)), result) \n",
        "  if i+1 == result_num:\n",
        "    break\n",
        "  feature_collection.clear()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "GT : 5, Pred : 3, Prob : 0.50\n",
            "GT : 9, Pred : 9, Prob : 0.78\n",
            "GT : 6, Pred : 6, Prob : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQbV17CADOL7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5d7a5680-baad-4219-ed2a-78a37dca7ff9"
      },
      "source": [
        "# CAM with my Dataset\n",
        "# Images should be in './data/OWN'\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "dataset = 'OWN'\n",
        "data_path = './data'\n",
        "model_path = './model'\n",
        "model_name = 'CAMnet.pth'\n",
        "result_path = './result'\n",
        "# img_size = 32\n",
        "img_size = 128\n",
        "result_num = 3\n",
        "\n",
        "if not os.path.exists(result_path):\n",
        "    os.mkdir(result_path)\n",
        "\n",
        "# test_loader, num_class = set_testloader(dataset, data_path, img_size)\n",
        "test_loader, _ = set_testloader(dataset, data_path, img_size)\n",
        "num_class = 10\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "net = CNN(img_size=img_size, num_class=num_class).to(device)\n",
        "net.load_state_dict(torch.load(os.path.join(model_path, model_name)))\n",
        "\n",
        "feature_collection = []\n",
        "def get_feature(input):\n",
        "  _, feature = net(input)\n",
        "  feature_collection.append(feature.cpu().data.numpy())\n",
        "\n",
        "params = list(net.parameters())\n",
        "weight_for_softmax = np.squeeze(params[-2].cpu().data.numpy())\n",
        "\n",
        "def Do_CAM(feature, weigth_for_softmax, class_id):\n",
        "  upsample_size = (img_size, img_size)\n",
        "  _, c, h, w = feature.shape\n",
        "  cam = np.dot(weight_for_softmax[class_id],feature.reshape(c, h*w))\n",
        "  cam = cam.reshape(h, w)\n",
        "  cam = (cam - np.min(cam)) \n",
        "  cam = cam / np.max(cam)\n",
        "  cam = np.uint8(255 * cam)\n",
        "  cam = cv2.resize(cam, upsample_size)\n",
        "  return cam\n",
        "\n",
        "for i, (image, label) in enumerate(test_loader):\n",
        "  PIL_image = transforms.ToPILImage()(image[0])\n",
        "  PIL_image.save(os.path.join(result_path, 'img%d.png' %(i+1)))\n",
        "  image, label = image.to(device), label.to(device)\n",
        "  get_feature(image)\n",
        "  out, _ = net(image)\n",
        "  Sc = F.softmax(out, dim=1).data.squeeze()\n",
        "  prob, id = Sc.sort(0,True)\n",
        "  print(\"GT : %d, Pred : %d, Prob : %.2f\" % (label.item(), id[0].item(), prob[0].item()))\n",
        "  CAM = Do_CAM(feature_collection[0], weight_for_softmax, id[0].item())\n",
        "  image = cv2.imread(os.path.join(result_path, 'img%d.png' % (i+1)))\n",
        "  height, width, _ = image.shape\n",
        "  heatmap = cv2.applyColorMap(cv2.resize(CAM, (width, height)), cv2.COLORMAP_JET)\n",
        "  result = heatmap*0.3 + image*0.5\n",
        "  cv2.imwrite(os.path.join(result_path, 'cam%d.png' % (i+1)), result) \n",
        "  if i+1 == result_num:\n",
        "    break\n",
        "  feature_collection.clear()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GT : 1, Pred : 3, Prob : 0.39\n",
            "GT : 1, Pred : 2, Prob : 0.73\n",
            "GT : 1, Pred : 2, Prob : 0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zd-kSQrE9O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}